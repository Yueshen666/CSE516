% CSE 516 Machine Learning% Q - learning coding example% by YZ% Consider the following gridworld:% + - - - - + - - - - + - - - - +% |         |         |         |% |  START  |    2    |    3    |% |    1    |         |         |% + - - - - + - - - - + - - - - +% |         |         |         |% |    4    | KEEPOUT |  GOAL.  |% |         |    5    |    6    |% + - - - - + - - - - + - - - - +% transitions. 0 means invalid transition.T = [ 0 4 0 2 ;       0 5 1 3 ;      0 6 2 0 ;      1 0 0 5 ;      2 0 4 6 ;      3 0 5 0 ];% reward in single vector since we have the transition matrix defined separatly% and R(s'|s,a) is equivalent to R(s')R = [-1,-1,-1,-1,-10,10]; % Q matrixQ = zeros(6,4);% discount rate : gamma gamma = 0.8;for i=1:10  ss = 1 ; % start from the start state  curr = ss; % current state  while curr != 6    % pick a random action, and end up in a random valid state.    possibiles = T(curr,:);    [action, nxt_state] = random_choice(possibiles);    % immediate reward:    imm_r = R(nxt_state);        % this block is to calculate gamma*max(Q'(s',a'))    future_rewards = [];    future_states = T(nxt_state,:);    for a = 1:length(future_states)      expected_future_state = future_states(a);      if expected_future_state != 0        future_rewards = [future_rewards Q(nxt_state, a)];      end    end    discounted_max_future_reward = gamma*max(future_rewards);    % end of block        % update Q value:       q_value = imm_r + discounted_max_future_reward;    Q(curr,action) = q_value;                curr = nxt_state;  end    display(Q);  pause(1);  end                                    